"""
Evaluation module for the Enhanced MLOps Framework for Agentic AI RAG Workflows.

This module provides functionality for evaluating RAG system performance,
including relevance, faithfulness, and answer quality for medical customer support scenarios.
"""

import os
import logging
from typing import Dict, Any, Optional, List, Union
import time
import json
import re
from pydantic import BaseModel, Field

from ..core import FrameworkException, ServiceRegistry, MetricsCollector
from ..core.config import EvaluationConfig, ConfigManager

logger = logging.getLogger(__name__)

class EvaluationRequest(BaseModel):
    """Model for an evaluation request."""
    
    query: str = Field(..., description="User query that was processed")
    response: str = Field(..., description="Response generated by the system")
    retrieved_documents: List[Dict[str, Any]] = Field(default_factory=list, description="Documents retrieved for the query")
    ground_truth: Optional[str] = Field(None, description="Ground truth answer if available")
    evaluation_types: List[str] = Field(default_factory=list, description="Types of evaluations to perform")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional metadata for the evaluation")

class EvaluationResult(BaseModel):
    """Model for an evaluation result."""
    
    query: str = Field(..., description="User query that was evaluated")
    scores: Dict[str, float] = Field(default_factory=dict, description="Evaluation scores by metric")
    issues: List[Dict[str, Any]] = Field(default_factory=list, description="Issues identified during evaluation")
    processing_time: float = Field(..., description="Time taken to process the evaluation in seconds")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Metadata about the evaluation")

class EvaluationService:
    """Service for evaluating RAG system performance."""
    
    def __init__(self, config: EvaluationConfig = None):
        """Initialize the evaluation service with configuration."""
        if config is None:
            config_manager = ConfigManager()
            app_config = config_manager.get_config()
            config = app_config.evaluation
        
        self.config = config
        self.metrics = MetricsCollector()
        
        # Register with service registry
        service_registry = ServiceRegistry()
        service_registry.register("evaluation_service", self)
        
        logger.info("Evaluation Service initialized")
    
    def evaluate(self, request: EvaluationRequest) -> EvaluationResult:
        """Evaluate a RAG system response."""
        start_time = time.time()
        
        try:
            scores = {}
            issues = []
            
            # Determine which evaluations to perform
            eval_types = request.evaluation_types
            if not eval_types:
                # Use default evaluations based on configuration
                if self.config.relevance_enabled:
                    eval_types.append("relevance")
                if self.config.faithfulness_enabled:
                    eval_types.append("faithfulness")
                if self.config.answer_quality_enabled:
                    eval_types.append("answer_quality")
                if self.config.medical_accuracy_enabled:
                    eval_types.append("medical_accuracy")
            
            # Perform requested evaluations
            for eval_type in eval_types:
                if eval_type == "relevance":
                    relevance_result = self._evaluate_relevance(request)
                    scores.update(relevance_result["scores"])
                    issues.extend(relevance_result["issues"])
                
                elif eval_type == "faithfulness":
                    faithfulness_result = self._evaluate_faithfulness(request)
                    scores.update(faithfulness_result["scores"])
                    issues.extend(faithfulness_result["issues"])
                
                elif eval_type == "answer_quality":
                    quality_result = self._evaluate_answer_quality(request)
                    scores.update(quality_result["scores"])
                    issues.extend(quality_result["issues"])
                
                elif eval_type == "medical_accuracy":
                    medical_result = self._evaluate_medical_accuracy(request)
                    scores.update(medical_result["scores"])
                    issues.extend(medical_result["issues"])
            
            # Record metrics
            processing_time = time.time() - start_time
            self.metrics.record(
                "evaluation_duration",
                processing_time,
                {"eval_types": ",".join(eval_types)}
            )
            
            # Create and return result
            result = EvaluationResult(
                query=request.query,
                scores=scores,
                issues=issues,
                processing_time=processing_time,
                metadata={
                    "eval_types": eval_types,
                    "issue_count": len(issues)
                }
            )
            
            return result
            
        except Exception as e:
            logger.error(f"Error evaluating response: {e}")
            
            # Record error metric
            self.metrics.record(
                "evaluation_error",
                1,
                {"error_type": type(e).__name__}
            )
            
            raise FrameworkException(
                f"Failed to evaluate response: {str(e)}",
                code="EVALUATION_ERROR"
            )
    
    def _evaluate_relevance(self, request: EvaluationRequest) -> Dict[str, Any]:
        """Evaluate the relevance of retrieved documents to the query."""
        # This is a simplified implementation
        # In a real system, this would use more sophisticated relevance evaluation
        
        scores = {}
        issues = []
        
        # Check if there are any retrieved documents
        if not request.retrieved_documents:
            issues.append({
                "type": "relevance",
                "subtype": "no_documents",
                "description": "No documents were retrieved for the query",
                "severity": "high"
            })
            scores["relevance"] = 0.0
            return {"scores": scores, "issues": issues}
        
        # Calculate relevance score based on query term overlap
        query_terms = set(request.query.lower().split())
        
        # Calculate relevance for each document
        doc_relevance_scores = []
        for i, doc in enumerate(request.retrieved_documents):
            doc_text = doc.get("text", "")
            doc_terms = set(doc_text.lower().split())
            
            # Calculate term overlap
            if query_terms and doc_terms:
                overlap = len(query_terms.intersection(doc_terms)) / len(query_terms)
            else:
                overlap = 0.0
            
            # Apply position bias (documents retrieved earlier should be more relevant)
            position_weight = 1.0 / (i + 1)
            doc_score = overlap * position_weight
            doc_relevance_scores.append(doc_score)
            
            # Check for low relevance
            if doc_score < 0.2:
                issues.append({
                    "type": "relevance",
                    "subtype": "low_relevance",
                    "description": f"Document at position {i+1} has low relevance to the query",
                    "document_id": doc.get("id", f"doc_{i}"),
                    "score": doc_score,
                    "severity": "medium"
                })
        
        # Calculate overall relevance score
        if doc_relevance_scores:
            # Weight earlier documents more heavily
            weights = [1.0 / (i + 1) for i in range(len(doc_relevance_scores))]
            weighted_sum = sum(score * weight for score, weight in zip(doc_relevance_scores, weights))
            weight_sum = sum(weights)
            relevance_score = weighted_sum / weight_sum if weight_sum > 0 else 0.0
        else:
            relevance_score = 0.0
        
        scores["relevance"] = relevance_score
        
        # Check for overall low relevance
        if relevance_score < 0.3:
            issues.append({
                "type": "relevance",
                "subtype": "overall_low_relevance",
                "description": "Overall relevance of retrieved documents is low",
                "score": relevance_score,
                "severity": "high"
            })
        
        logger.debug(f"Relevance evaluation: {relevance_score:.2f}")
        return {"scores": scores, "issues": issues}
    
    def _evaluate_faithfulness(self, request: EvaluationRequest) -> Dict[str, Any]:
        """Evaluate the faithfulness of the response to the retrieved documents."""
        # This is a simplified implementation
        # In a real system, this would use more sophisticated faithfulness evaluation
        
        scores = {}
        issues = []
        
        # Check if there are any retrieved documents
        if not request.retrieved_documents:
            issues.append({
                "type": "faithfulness",
                "subtype": "no_documents",
                "description": "No documents were retrieved to evaluate faithfulness",
                "severity": "high"
            })
            scores["faithfulness"] = 0.0
            return {"scores": scores, "issues": issues}
        
        # Extract key statements from the response
        response_statements = self._extract_statements(request.response)
        
        # Check each statement for support in the retrieved documents
        supported_statements = 0
        unsupported_statements = []
        
        for statement in response_statements:
            supported = False
            
            # Check if the statement is supported by any retrieved document
            for doc in request.retrieved_documents:
                doc_text = doc.get("text", "")
                
                # Simple check: is the statement a substring of the document?
                # In a real system, this would use more sophisticated entailment checking
                if statement.lower() in doc_text.lower():
                    supported = True
                    break
            
            if supported:
                supported_statements += 1
            else:
                unsupported_statements.append(statement)
        
        # Calculate faithfulness score
        if response_statements:
            faithfulness_score = supported_statements / len(response_statements)
        else:
            faithfulness_score = 1.0  # No statements to check
        
        scores["faithfulness"] = faithfulness_score
        
        # Check for unsupported statements
        for statement in unsupported_statements:
            issues.append({
                "type": "faithfulness",
                "subtype": "unsupported_statement",
                "description": "Statement in response is not supported by retrieved documents",
                "statement": statement,
                "severity": "high"
            })
        
        # Check for overall low faithfulness
        if faithfulness_score < 0.7:
            issues.append({
                "type": "faithfulness",
                "subtype": "overall_low_faithfulness",
                "description": "Overall faithfulness of response is low",
                "score": faithfulness_score,
                "severity": "high"
            })
        
        logger.debug(f"Faithfulness evaluation: {faithfulness_score:.2f}")
        return {"scores": scores, "issues": issues}
    
    def _evaluate_answer_quality(self, request: EvaluationRequest) -> Dict[str, Any]:
        """Evaluate the quality of the answer."""
        # This is a simplified implementation
        # In a real system, this would use more sophisticated answer quality evaluation
        
        scores = {}
        issues = []
        
        # Check response length
        response_length = len(request.response.split())
        if response_length < 10:
            issues.append({
                "type": "answer_quality",
                "subtype": "too_short",
                "description": "Response is too short",
                "length": response_length,
                "severity": "medium"
            })
            scores["conciseness"] = 1.0  # Short answers are concise
            scores["completeness"] = 0.2  # Short answers are likely incomplete
        elif response_length > 300:
            issues.append({
                "type": "answer_quality",
                "subtype": "too_long",
                "description": "Response is too long",
                "length": response_length,
                "severity": "low"
            })
            scores["conciseness"] = 0.3  # Long answers are not concise
            scores["completeness"] = 0.9  # Long answers are likely complete
        else:
            scores["conciseness"] = 0.8  # Reasonable length
            scores["completeness"] = 0.8  # Reasonable length
        
        # Check for coherence
        coherence_score = self._evaluate_coherence(request.response)
        scores["coherence"] = coherence_score
        
        if coherence_score < 0.5:
            issues.append({
                "type": "answer_quality",
                "subtype": "low_coherence",
                "description": "Response lacks coherence",
                "score": coherence_score,
                "severity": "medium"
            })
        
        # Check for ground truth if available
        if request.ground_truth:
            accuracy_score = self._evaluate_accuracy(request.response, request.ground_truth)
            scores["accuracy"] = accuracy_score
            
            if accuracy_score < 0.5:
                issues.append({
                    "type": "answer_quality",
                    "subtype": "low_accuracy",
                    "description": "Response has low accuracy compared to ground truth",
                    "score": accuracy_score,
                    "severity": "high"
                })
        
        # Calculate overall quality score
        quality_scores = [
            scores.get("conciseness", 0.0),
            scores.get("completeness", 0.0),
            scores.get("coherence", 0.0)
        ]
        if "accuracy" in scores:
            quality_scores.append(scores["accuracy"])
        
        overall_quality = sum(quality_scores) / len(quality_scores)
        scores["overall_quality"] = overall_quality
        
        logger.debug(f"Answer quality evaluation: {overall_quality:.2f}")
        return {"scores": scores, "issues": issues}
    
    def _evaluate_medical_accuracy(self, request: EvaluationRequest) -> Dict[str, Any]:
        """Evaluate the medical accuracy of the response."""
        # This is a simplified implementation
        # In a real system, this would use more sophisticated medical accuracy evaluation
        
        scores = {}
        issues = []
        
        # Check for medical terms in the response
        medical_terms = self._extract_medical_terms(request.response)
        
        if not medical_terms:
            issues.append({
                "type": "medical_accuracy",
                "subtype": "no_medical_terms",
                "description": "Response does not contain medical terminology",
                "severity": "medium"
            })
            scores["medical_terminology"] = 0.0
        else:
            scores["medical_terminology"] = 0.8  # Presence of medical terms is good
        
        # Check for medical disclaimers
        has_disclaimer = self._check_medical_disclaimer(request.response)
        scores["has_disclaimer"] = 1.0 if has_disclaimer else 0.0
        
        if not has_disclaimer:
            issues.append({
                "type": "medical_accuracy",
                "subtype": "missing_disclaimer",
                "description": "Response is missing appropriate medical disclaimers",
                "severity": "medium"
            })
        
        # Check for contradictions with retrieved documents
        contradiction_score = self._check_medical_contradictions(request.response, request.retrieved_documents)
        scores["contradiction_free"] = contradiction_score
        
        if contradiction_score < 0.7:
            issues.append({
                "type": "medical_accuracy",
                "subtype": "potential_contradictions",
                "description": "Response may contradict medical information in retrieved documents",
                "score": contradiction_score,
                "severity": "high"
            })
        
        # Calculate overall medical accuracy score
        medical_scores = [
            scores.get("medical_terminology", 0.0),
            scores.get("has_disclaimer", 0.0),
            scores.get("contradiction_free", 0.0)
        ]
        
        overall_medical_accuracy = sum(medical_scores) / len(medical_scores)
        scores["medical_accuracy"] = overall_medical_accuracy
        
        logger.debug(f"Medical accuracy evaluation: {overall_medical_accuracy:.2f}")
        return {"scores": scores, "issues": issues}
    
    def _extract_statements(self, text: str) -> List[str]:
        """Extract key statements from text."""
        # This is a simplified implementation
        # In a real system, this would use more sophisticated statement extraction
        
        # Split by sentence-ending punctuation
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        # Filter out very short sentences
        statements = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        return statements
    
    def _evaluate_coherence(self, text: str) -> float:
        """Evaluate the coherence of text."""
        # This is a simplified implementation
        # In a real system, this would use more sophisticated coherence evaluation
        
        # Split into sentences
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        if len(sentences) <= 1:
            return 1.0  # Single sentence is coherent by default
        
        # Check for coherence markers
        coherence_markers = [
            "therefore", "thus", "consequently", "as a result", "because",
            "since", "additionally", "moreover", "furthermore", "however",
            "nevertheless", "although", "despite", "in contrast", "similarly",
            "for example", "specifically", "in particular", "in summary", "in conclusion"
        ]
        
        marker_count = 0
        for sentence in sentences:
            for marker in coherence_markers:
                if marker in sentence.lower():
                    marker_count += 1
                    break
        
        # Calculate coherence score based on presence of markers
        coherence_score = min(1.0, marker_count / (len(sentences) - 1))
        
        # Adjust score based on sentence length variation
        sentence_lengths = [len(s.split()) for s in sentences]
        length_variation = max(sentence_lengths) / (sum(sentence_lengths) / len(sentence_lengths)) if sentence_lengths else 1.0
        
        # Penalize extreme variation
        if length_variation > 3.0:
            coherence_score *= 0.8
        
        return coherence_score
    
    def _evaluate_accuracy(self, response: str, ground_truth: str) -> float:
        """Evaluate the accuracy of a response against ground truth."""
        # This is a simplified implementation
        # In a real system, this would use more sophisticated accuracy evaluation
        
        # Convert to lowercase for comparison
        response_lower = response.lower()
        ground_truth_lower = ground_truth.lower()
        
        # Extract key terms from ground truth
        ground_truth_terms = set(ground_truth_lower.split())
        
        # Count matching terms in response
        matching_terms = 0
        for term in ground_truth_terms:
            if term in response_lower:
                matching_terms += 1
        
        # Calculate accuracy score
        if ground_truth_terms:
            accuracy_score = matching_terms / len(ground_truth_terms)
        else:
            accuracy_score = 0.0
        
        return accuracy_score
    
    def _extract_medical_terms(self, text: str) -> List[str]:
        """Extract medical terms from text."""
        # This is a simplified implementation
        # In a real system, this would use a medical terminology database
        
        medical_terms = []
        
        # Common medical term patterns
        medical_patterns = [
            r"\b(?:diagnosis|prognosis|treatment|symptom|condition|disease|disorder|syndrome)\b",
            r"\b(?:medication|prescription|dosage|antibiotic|analgesic|anti-inflammatory)\b",
            r"\b(?:hypertension|diabetes|asthma|arthritis|depression|anxiety)\b",
            r"\b(?:cardiac|pulmonary|renal|hepatic|neurological|gastrointestinal)\b"
        ]
        
        for pattern in medical_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                medical_terms.append(match.group(0))
        
        return medical_terms
    
    def _check_medical_disclaimer(self, text: str) -> bool:
        """Check if text contains appropriate medical disclaimers."""
        # This is a simplified implementation
        # In a real system, this would use more sophisticated disclaimer detection
        
        disclaimer_patterns = [
            r"(?:not medical advice|not a substitute for professional medical advice)",
            r"(?:consult (?:your|a) (?:doctor|physician|healthcare provider))",
            r"(?:seek (?:professional|medical) advice)",
            r"(?:information is for educational purposes only)"
        ]
        
        for pattern in disclaimer_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        
        return False
    
    def _check_medical_contradictions(self, response: str, documents: List[Dict[str, Any]]) -> float:
        """Check for contradictions between response and retrieved documents."""
        # This is a simplified implementation
        # In a real system, this would use more sophisticated contradiction detection
        
        if not documents:
            return 1.0  # No documents to check against
        
        # Extract medical statements from response
        response_statements = self._extract_statements(response)
        medical_statements = []
        
        for statement in response_statements:
            if any(term in statement.lower() for term in self._extract_medical_terms(statement)):
                medical_statements.append(statement)
        
        if not medical_statements:
            return 1.0  # No medical statements to check
        
        # Check for potential contradictions
        contradiction_count = 0
        
        for statement in medical_statements:
            statement_lower = statement.lower()
            
            # Check for negation patterns that might indicate contradictions
            negation_patterns = [
                r"\bnot\b", r"\bno\b", r"\bnever\b", r"\bcontrary\b",
                r"\bunlike\b", r"\bopposed\b", r"\bdisagree\b", r"\bincorrect\b"
            ]
            
            for doc in documents:
                doc_text = doc.get("text", "").lower()
                
                # Check if statement appears in document with negation
                for pattern in negation_patterns:
                    if re.search(f"{pattern}.*{re.escape(statement_lower)}", doc_text) or \
                       re.search(f"{re.escape(statement_lower)}.*{pattern}", doc_text):
                        contradiction_count += 1
                        break
        
        # Calculate contradiction-free score
        contradiction_free_score = 1.0 - (contradiction_count / len(medical_statements))
        
        return contradiction_free_score
    
    def health_check(self) -> Dict[str, Any]:
        """Perform a health check on the evaluation service."""
        status = "healthy"
        message = "Evaluation Service is healthy"
        
        try:
            # Perform a simple evaluation
            request = EvaluationRequest(
                query="What are the symptoms of a cold?",
                response="Common cold symptoms include runny nose, sore throat, and cough.",
                retrieved_documents=[
                    {"id": "doc1", "text": "Cold symptoms include runny nose, sore throat, cough, and mild fever."}
                ]
            )
            self.evaluate(request)
        except Exception as e:
            status = "unhealthy"
            message = f"Health check failed: {str(e)}"
        
        return {
            "status": status,
            "message": message,
            "timestamp": time.time(),
            "details": {
                "relevance_enabled": self.config.relevance_enabled,
                "faithfulness_enabled": self.config.faithfulness_enabled,
                "answer_quality_enabled": self.config.answer_quality_enabled,
                "medical_accuracy_enabled": self.config.medical_accuracy_enabled
            }
        }

# Initialize global instance
evaluation_service = None

def get_evaluation_service():
    """Get or create the evaluation service instance."""
    global evaluation_service
    if evaluation_service is None:
        evaluation_service = EvaluationService()
    return evaluation_service
